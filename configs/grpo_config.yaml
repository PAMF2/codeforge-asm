project:
  name: codeforge-asm
  seed: 42

model:
  name_or_path: mistralai/Ministral-8B-Instruct-2410
  trust_remote_code: true
  load_in_4bit: true
  lora_r: 16
  lora_alpha: 32
  lora_dropout: 0.05

training:
  grpo_backend: manual # options: manual | trl
  iterations: 10
  prompts_per_iteration: 20   # random sample from full dataset each iteration
  use_random_sampling: true   # random vs deterministic (first N)
  generations_per_prompt: 16
  learning_rate: 5e-6
  batch_size: 2
  gradient_accumulation_steps: 4
  grad_clip_norm: 1.0
  train_max_seq_len: 1024
  max_new_tokens: 512
  temperature: 0.8
  top_p: 0.95
  kl_beta: 0.1
  use_mcts_after_iteration: 2
  use_wandb: true
  push_to_hub: true
  hub_repo_id: mistral-hackaton-2026/codeforge
  hub_fallback_repo_id: PAMF2/codeforge
  hub_private: true
  dry_run: true
  use_unsloth: false  # set true if unsloth is installed

reward:
  stage_weights:
    assemble: 0.25
    link: 0.25
    run: 0.20
    correctness: 0.30
  timeout_seconds: 5

mcts:
  simulations: 32
  max_lines: 30
  branch_factor: 4
  exploration_constant: 1.414
  max_depth: 15
  min_tier: 3  # Only apply MCTS to tier >= 3

paths:
  prompt_dataset: prompts/dataset.json
  artifacts_dir: artifacts
  checkpoints_dir: checkpoints
